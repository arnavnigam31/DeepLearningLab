{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7YdJyiMF2AXu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67Z__rHe4ndE"
      },
      "source": [
        "# Why tensor?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrtmSAuL43Kf"
      },
      "source": [
        "We can have inbuilt list, numpy array and torch arrays, so we can compare there performance using time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roONFvX85E-4",
        "outputId": "5f01ecd9-316c-482f-a090-8d6b2055f4a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.00496220588684082\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "size=10000\n",
        "\n",
        "list1 = list(range(size))\n",
        "list2 = list(range(size))\n",
        "\n",
        "start_time = time.time();\n",
        "list3 = [a+b for a,b in zip(list1,list2)]\n",
        "end_time = time.time()\n",
        "\n",
        "print(end_time-start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1468b65",
        "outputId": "5e14aa42-b649-4769-ab85-77fe7a71ac37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.00014257431030273438\n"
          ]
        }
      ],
      "source": [
        "size=10000\n",
        "\n",
        "list1 = np.arange(size)\n",
        "list2 = np.arange(size)\n",
        "\n",
        "start_time = time.time();\n",
        "list3 = list1 + list2\n",
        "end_time = time.time()\n",
        "\n",
        "print(end_time-start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2acf29b2",
        "outputId": "3d753d27-12db-42b1-dd46-06a8fd4b996f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0001125335693359375\n"
          ]
        }
      ],
      "source": [
        "tensor1 = torch.arange(size)\n",
        "tensor2 = torch.arange(size)\n",
        "\n",
        "start_time = time.time()\n",
        "result_torch = tensor1 + tensor2\n",
        "end_time = time.time()\n",
        "torch_time = end_time - start_time\n",
        "print(torch_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3d5fbea"
      },
      "source": [
        "As we can observe from the output, NumPy arrays and PyTorch tensors are significantly faster for element-wise multiplication compared to standard Python lists. This is because they are implemented in C/C++ and optimized for numerical operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEveVjaU6w8-"
      },
      "source": [
        "# Create 1D, 2D and 3D array using numpy and torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2f83751",
        "outputId": "7aa32c11-c456-434c-aeab-3da6f5e28e07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3, 4])\n",
            "torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "# 1D array using torch\n",
        "# to make torch.tensor([elements])\n",
        "\n",
        "arr = torch.tensor([1,2,3,4])\n",
        "print(arr)\n",
        "print(arr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6a0f0e3",
        "outputId": "b1e8f85a-d7ff-46af-c725-b6bea677937f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n",
            "torch.Size([2, 4])\n"
          ]
        }
      ],
      "source": [
        "# 2D array using torch\n",
        "arr = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
        "print(arr)\n",
        "print(arr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ad3737",
        "outputId": "5f4dea5f-9e6c-4f99-b5a5-0bbfffd129b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n",
            "torch.Size([2, 2, 2])\n"
          ]
        }
      ],
      "source": [
        "# 3D array using torch\n",
        "arr = torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
        "print(arr)\n",
        "print(arr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42b49c30",
        "outputId": "abbdc317-5930-4e4c-c176-2b8fa0eded41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 2 3 4]\n",
            "(4,)\n"
          ]
        }
      ],
      "source": [
        "# 1D array numpy\n",
        "arr = np.array([1,2,3,4])\n",
        "print(arr)\n",
        "print(arr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33381b6f",
        "outputId": "51608c2a-b2b8-4e0a-883e-c5854168b767"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 2 3 4]\n",
            " [5 6 7 8]]\n",
            "(2, 4)\n"
          ]
        }
      ],
      "source": [
        "# 2D array numpy\n",
        "arr = np.array([[1,2,3,4],[5,6,7,8]])\n",
        "print(arr)\n",
        "print(arr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6ad45fc",
        "outputId": "fd09c066-6549-40b8-b01d-3d0b6ddd1b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[1 2]\n",
            "  [3 4]]\n",
            "\n",
            " [[5 6]\n",
            "  [7 8]]]\n",
            "(2, 2, 2)\n"
          ]
        }
      ],
      "source": [
        "# 3D array numpy\n",
        "arr = np.array([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
        "print(arr)\n",
        "print(arr.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWOfPA8U7kP3"
      },
      "source": [
        "# Show basic operations : Element wise operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg2Gd58W7mjL",
        "outputId": "21675e6c-d3a6-469a-bffc-1d5a483cd789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Addition  tensor([ 6,  8, 10, 12])\n",
            "subtraction  tensor([-4, -4, -4, -4])\n",
            "Multiply  tensor([ 5, 12, 21, 32])\n",
            "Divide  tensor([0.2000, 0.3333, 0.4286, 0.5000])\n"
          ]
        }
      ],
      "source": [
        "arr1 = torch.tensor([1,2,3,4])\n",
        "arr2 = torch.tensor([5,6,7,8])\n",
        "\n",
        "print(\"Addition \", arr1+arr2)\n",
        "print(\"subtraction \", arr1-arr2)\n",
        "print(\"Multiply \", arr1*arr2)\n",
        "print(\"Divide \", arr1/arr2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df1cbddc"
      },
      "source": [
        "### Indexing and Slicing with PyTorch Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7af3d4df",
        "outputId": "ac7eda01-d4b2-486e-c362-7a8c3e43529f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Tensor:\n",
            "tensor([[ 1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8],\n",
            "        [ 9, 10, 11, 12]])\n",
            "Element at (0, 1): tensor(2)\n",
            "First row: tensor([1, 2, 3, 4])\n",
            "Second column: tensor([ 2,  6, 10])\n",
            "\n",
            "First two rows:\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n",
            "\n",
            "Last two columns:\n",
            "tensor([[ 3,  4],\n",
            "        [ 7,  8],\n",
            "        [11, 12]])\n",
            "Boolean Mask (elements > 5):\n",
            "tensor([[False, False, False,  True],\n",
            "        [ True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True]])\n",
            "Elements greater than 5: tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
            "\n",
            "Subtensor (rows 1 onwards, cols 1-2):\n",
            "tensor([[ 6,  7],\n",
            "        [10, 11]])\n"
          ]
        }
      ],
      "source": [
        "arr = torch.tensor([[1, 2, 3, 4],[5, 6, 7, 8],[9, 10, 11, 12]])\n",
        "print(\"Original Tensor:\")\n",
        "print(arr)\n",
        "\n",
        "# Indexing\n",
        "print(\"Element at (0, 1):\", arr[0, 1])\n",
        "print(\"First row:\", arr[0])\n",
        "print(\"Second column:\", arr[:, 1])\n",
        "\n",
        "# Slicing\n",
        "print(\"\\nFirst two rows:\")\n",
        "print(arr[0:2, :])\n",
        "print(\"\\nLast two columns:\")\n",
        "print(arr[:, 2:])\n",
        "\n",
        "# Boolean Masking\n",
        "mask = arr > 3\n",
        "print(\"Boolean Mask (elements > 5):\")\n",
        "print(mask)\n",
        "print(\"Elements greater than 5:\", arr[mask])\n",
        "\n",
        "# Extracting Subtensors\n",
        "subtensor_rows_cols = arr[1:, 1:3]\n",
        "print(\"\\nSubtensor (rows 1 onwards, cols 1-2):\")\n",
        "print(subtensor_rows_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c6cadca"
      },
      "source": [
        "### Understanding PyTorch Tensor Manipulation Functions\n",
        "\n",
        "PyTorch provides several functions to manipulate the shape of tensors without changing their underlying data. These are crucial for preparing data for neural networks, which often require specific input dimensions.\n",
        "\n",
        "#### 1. `torch.view()`\n",
        "\n",
        "*   **Use:** Returns a new tensor with the same data as the `self` tensor but with a different shape. The new tensor shares the same underlying data with the original tensor, meaning changes to one will affect the other. This operation is only possible if the new view is **contiguous** in memory. If the tensor is not contiguous (e.g., after transposing), you might need to use `.contiguous()` first.\n",
        "*   **When to use:** When you need a reinterpretation of the tensor's shape and the tensor's memory layout is compatible. It's generally faster than `.reshape()` if contiguity is already met.\n",
        "\n",
        "#### 2. `torch.reshape()`\n",
        "\n",
        "*   **Use:** Returns a tensor with the same data and number of elements as `self` but with the specified shape. Unlike `.view()`, `reshape()` can handle non-contiguous tensors by creating a copy if necessary. If the tensor is contiguous, it behaves like `.view()`.\n",
        "*   **When to use:** When you need to change a tensor's shape and you're not sure if it's contiguous, or if you don't want to worry about contiguity. It's more flexible than `.view()`.\n",
        "\n",
        "#### 3. `torch.unsqueeze()`\n",
        "\n",
        "*   **Use:** Returns a new tensor with a dimension of size one inserted at the specified position (`dim`). This increases the number of dimensions of the tensor.\n",
        "*   **When to use:** To add a new dimension to a tensor, often for operations that require a specific number of dimensions (e.g., adding a batch dimension to a single image or adding a channel dimension).\n",
        "\n",
        "#### 4. `torch.squeeze()`\n",
        "\n",
        "*   **Use:** Returns a new tensor with all the dimensions of size 1 removed. If a `dim` argument is provided, only dimensions of size 1 at that specific position are removed.\n",
        "*   **When to use:** To remove singleton dimensions (dimensions with size 1) from a tensor. This is useful for cleaning up tensor shapes after operations like `unsqueeze()` or when dealing with model outputs that have unnecessary dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b35f9b33",
        "outputId": "895b2be7-7c1b-46ee-913a-904484ab5510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor:\n",
            "tensor([[[ 0.6373,  0.4478, -0.5319, -0.8468],\n",
            "         [ 0.0129, -0.8319, -2.1778,  1.2889],\n",
            "         [-0.3251,  0.7576, -0.3509, -1.0392]],\n",
            "\n",
            "        [[ 0.4122, -0.6218,  0.0513,  1.7550],\n",
            "         [-0.3739,  1.0608, -1.2845,  1.0495],\n",
            "         [-1.1919,  1.7971,  0.9187, -0.1452]]])\n",
            "Shape of x: torch.Size([2, 3, 4])\n",
            "Tensor after .view(2, 12):\n",
            "tensor([[ 0.6373,  0.4478, -0.5319, -0.8468,  0.0129, -0.8319, -2.1778,  1.2889,\n",
            "         -0.3251,  0.7576, -0.3509, -1.0392],\n",
            "        [ 0.4122, -0.6218,  0.0513,  1.7550, -0.3739,  1.0608, -1.2845,  1.0495,\n",
            "         -1.1919,  1.7971,  0.9187, -0.1452]])\n",
            "Shape of x_view: torch.Size([2, 12])\n",
            "Tensor after .view(-1):\n",
            "tensor([ 0.6373,  0.4478, -0.5319, -0.8468,  0.0129, -0.8319, -2.1778,  1.2889,\n",
            "        -0.3251,  0.7576, -0.3509, -1.0392,  0.4122, -0.6218,  0.0513,  1.7550,\n",
            "        -0.3739,  1.0608, -1.2845,  1.0495, -1.1919,  1.7971,  0.9187, -0.1452])\n",
            "Shape of x_view_1d: torch.Size([24])\n",
            "\n",
            "RuntimeError with non-contiguous tensor: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
            "Shape of x_T_view (after contiguous): torch.Size([4, 6])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3503947550.py:21: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4416.)\n",
            "  x_T = x.T # Transpose makes it non-contiguous\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x = torch.randn(2, 3, 4)\n",
        "print(\"Original tensor:\")\n",
        "print(x)\n",
        "print(\"Shape of x:\", x.shape)\n",
        "\n",
        "\n",
        "x_view = x.view(2, 12)\n",
        "print(\"Tensor after .view(2, 12):\")\n",
        "print(x_view)\n",
        "print(\"Shape of x_view:\", x_view.shape)\n",
        "\n",
        "\n",
        "x_view_1d = x.view(-1)\n",
        "print(\"Tensor after .view(-1):\")\n",
        "print(x_view_1d)\n",
        "print(\"Shape of x_view_1d:\", x_view_1d.shape)\n",
        "\n",
        "# Note: .view() requires the tensor to be contiguous.\n",
        "# If x was non-contiguous, x.contiguous().view(...) would be needed.\n",
        "# Example of creating a non-contiguous tensor:\n",
        "x_T = x.T # Transpose makes it non-contiguous\n",
        "try:\n",
        "    x_T.view(4, 6)\n",
        "except RuntimeError as e:\n",
        "    print(f\"\\nRuntimeError with non-contiguous tensor: {e}\")\n",
        "x_T_view = x_T.contiguous().view(4, 6)\n",
        "print(\"Shape of x_T_view (after contiguous):\", x_T_view.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "970da7b3",
        "outputId": "6939cda4-2c92-4320-ef24-7a0f6beb6246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor (x):\n",
            "tensor([[[ 0.7841, -0.4514,  0.7814,  0.9869],\n",
            "         [ 1.2444, -0.4917, -0.8139,  0.7750],\n",
            "         [ 0.9569,  0.5631, -0.0483, -0.5326]],\n",
            "\n",
            "        [[-0.0498,  0.9727,  2.4889,  0.7373],\n",
            "         [ 0.9646,  1.6621,  0.7671,  1.6172],\n",
            "         [-0.6935, -0.8581, -0.3492,  0.3343]]])\n",
            "Shape of x: torch.Size([2, 3, 4])\n",
            "\n",
            "Tensor after .reshape(2, 12):\n",
            "tensor([[ 0.7841, -0.4514,  0.7814,  0.9869,  1.2444, -0.4917, -0.8139,  0.7750,\n",
            "          0.9569,  0.5631, -0.0483, -0.5326],\n",
            "        [-0.0498,  0.9727,  2.4889,  0.7373,  0.9646,  1.6621,  0.7671,  1.6172,\n",
            "         -0.6935, -0.8581, -0.3492,  0.3343]])\n",
            "Shape of x_reshape: torch.Size([2, 12])\n",
            "\n",
            "Tensor after .reshape(-1):\n",
            "tensor([ 0.7841, -0.4514,  0.7814,  0.9869,  1.2444, -0.4917, -0.8139,  0.7750,\n",
            "         0.9569,  0.5631, -0.0483, -0.5326, -0.0498,  0.9727,  2.4889,  0.7373,\n",
            "         0.9646,  1.6621,  0.7671,  1.6172, -0.6935, -0.8581, -0.3492,  0.3343])\n",
            "Shape of x_reshape_1d: torch.Size([24])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(2, 3, 4)\n",
        "print(\"Original tensor (x):\")\n",
        "print(x)\n",
        "print(\"Shape of x:\", x.shape)\n",
        "\n",
        "x_reshape = x.reshape(2, 12)\n",
        "print(\"\\nTensor after .reshape(2, 12):\")\n",
        "print(x_reshape)\n",
        "print(\"Shape of x_reshape:\", x_reshape.shape)\n",
        "\n",
        "\n",
        "x_reshape_1d = x.reshape(-1)\n",
        "print(\"\\nTensor after .reshape(-1):\")\n",
        "print(x_reshape_1d)\n",
        "print(\"Shape of x_reshape_1d:\", x_reshape_1d.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea6e9552",
        "outputId": "6c4877c0-aec4-43ae-c232-b8c0091f3791"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor (x):\n",
            "tensor([[-0.1789, -0.8893,  0.5231],\n",
            "        [-0.6281,  0.2372,  0.3681]])\n",
            "Shape of x: torch.Size([2, 3])\n",
            "\n",
            "Tensor after .unsqueeze(0):\n",
            "tensor([[[-0.1789, -0.8893,  0.5231],\n",
            "         [-0.6281,  0.2372,  0.3681]]])\n",
            "Shape of x_unsqueeze_0: torch.Size([1, 2, 3])\n",
            "\n",
            "Tensor after .unsqueeze(1):\n",
            "tensor([[[-0.1789, -0.8893,  0.5231]],\n",
            "\n",
            "        [[-0.6281,  0.2372,  0.3681]]])\n",
            "Shape of x_unsqueeze_1: torch.Size([2, 1, 3])\n",
            "\n",
            "Tensor after .unsqueeze(2):\n",
            "tensor([[[-0.1789],\n",
            "         [-0.8893],\n",
            "         [ 0.5231]],\n",
            "\n",
            "        [[-0.6281],\n",
            "         [ 0.2372],\n",
            "         [ 0.3681]]])\n",
            "Shape of x_unsqueeze_2: torch.Size([2, 3, 1])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(2, 3)\n",
        "print(\"Original tensor (x):\")\n",
        "print(x)\n",
        "print(\"Shape of x:\", x.shape)\n",
        "\n",
        "x_unsqueeze_0 = x.unsqueeze(0)\n",
        "print(\"Tensor after .unsqueeze(0):\")\n",
        "print(x_unsqueeze_0)\n",
        "print(\"Shape of x_unsqueeze_0:\", x_unsqueeze_0.shape)\n",
        "\n",
        "x_unsqueeze_1 = x.unsqueeze(1)\n",
        "print(\"Tensor after .unsqueeze(1):\")\n",
        "print(x_unsqueeze_1)\n",
        "print(\"Shape of x_unsqueeze_1:\", x_unsqueeze_1.shape)\n",
        "\n",
        "x_unsqueeze_2 = x.unsqueeze(2)\n",
        "print(\"Tensor after .unsqueeze(2):\")\n",
        "print(x_unsqueeze_2)\n",
        "print(\"Shape of x_unsqueeze_2:\", x_unsqueeze_2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "267af471",
        "outputId": "9ddd9abb-67ee-450a-d1e9-e0be85858a97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor (y):\n",
            "tensor([[[[[ 1.0541],\n",
            "           [-2.3625],\n",
            "           [-0.1122]]],\n",
            "\n",
            "\n",
            "         [[[ 0.2098],\n",
            "           [-0.6829],\n",
            "           [-0.0434]]],\n",
            "\n",
            "\n",
            "         [[[ 2.2127],\n",
            "           [-0.4295],\n",
            "           [-1.8247]]],\n",
            "\n",
            "\n",
            "         [[[-0.6139],\n",
            "           [-0.6861],\n",
            "           [ 0.2832]]],\n",
            "\n",
            "\n",
            "         [[[ 0.2986],\n",
            "           [-0.7364],\n",
            "           [ 0.3590]]]]])\n",
            "Shape of y: torch.Size([1, 5, 1, 3, 1])\n",
            "Tensor after .squeeze():\n",
            "tensor([[ 1.0541, -2.3625, -0.1122],\n",
            "        [ 0.2098, -0.6829, -0.0434],\n",
            "        [ 2.2127, -0.4295, -1.8247],\n",
            "        [-0.6139, -0.6861,  0.2832],\n",
            "        [ 0.2986, -0.7364,  0.3590]])\n",
            "Shape of y_squeeze_all: torch.Size([5, 3])\n",
            "Tensor after .squeeze(dim=2):\n",
            "tensor([[[[ 1.0541],\n",
            "          [-2.3625],\n",
            "          [-0.1122]],\n",
            "\n",
            "         [[ 0.2098],\n",
            "          [-0.6829],\n",
            "          [-0.0434]],\n",
            "\n",
            "         [[ 2.2127],\n",
            "          [-0.4295],\n",
            "          [-1.8247]],\n",
            "\n",
            "         [[-0.6139],\n",
            "          [-0.6861],\n",
            "          [ 0.2832]],\n",
            "\n",
            "         [[ 0.2986],\n",
            "          [-0.7364],\n",
            "          [ 0.3590]]]])\n",
            "Shape of y_squeeze_dim: torch.Size([1, 5, 3, 1])\n",
            "Tensor after .squeeze(dim=1):\n",
            "tensor([[[[[ 1.0541],\n",
            "           [-2.3625],\n",
            "           [-0.1122]]],\n",
            "\n",
            "\n",
            "         [[[ 0.2098],\n",
            "           [-0.6829],\n",
            "           [-0.0434]]],\n",
            "\n",
            "\n",
            "         [[[ 2.2127],\n",
            "           [-0.4295],\n",
            "           [-1.8247]]],\n",
            "\n",
            "\n",
            "         [[[-0.6139],\n",
            "           [-0.6861],\n",
            "           [ 0.2832]]],\n",
            "\n",
            "\n",
            "         [[[ 0.2986],\n",
            "           [-0.7364],\n",
            "           [ 0.3590]]]]])\n",
            "Shape of y_squeeze_non_singleton: torch.Size([1, 5, 1, 3, 1])\n"
          ]
        }
      ],
      "source": [
        "y = torch.randn(1, 5, 1, 3, 1)\n",
        "print(\"Original tensor (y):\")\n",
        "print(y)\n",
        "print(\"Shape of y:\", y.shape)\n",
        "\n",
        "y_squeeze_all = y.squeeze()\n",
        "print(\"Tensor after .squeeze():\")\n",
        "print(y_squeeze_all)\n",
        "print(\"Shape of y_squeeze_all:\", y_squeeze_all.shape)\n",
        "\n",
        "\n",
        "y_squeeze_dim = y.squeeze(dim=2)\n",
        "print(\"Tensor after .squeeze(dim=2):\")\n",
        "print(y_squeeze_dim)\n",
        "print(\"Shape of y_squeeze_dim:\", y_squeeze_dim.shape)\n",
        "\n",
        "y_squeeze_non_singleton = y.squeeze(dim=1)\n",
        "print(\"Tensor after .squeeze(dim=1):\")\n",
        "print(y_squeeze_non_singleton)\n",
        "print(\"Shape of y_squeeze_non_singleton:\", y_squeeze_non_singleton.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HEBIfOI0aeP"
      },
      "source": [
        "# Compare with numpy Reshape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "760ccad1"
      },
      "source": [
        "\n",
        "*   **Use:** Returns an array with the same data and number of elements as the original array but with a new shape. Unlike `torch.view()`, `numpy.reshape()` always returns a new view of the array if possible, otherwise it returns a copy. It handles non-contiguous arrays gracefully by making a copy if reshaping would require a change in memory layout.\n",
        "*   **When to use:** When you need to change the shape of a NumPy array. It's generally straightforward to use as it handles memory contiguity automatically, creating a copy only when necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d43277f",
        "outputId": "2c0d2106-be5d-4361-decb-5ce475221e4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original NumPy array (x_np):\n",
            "[[[ 0  1  2  3]\n",
            "  [ 4  5  6  7]\n",
            "  [ 8  9 10 11]]\n",
            "\n",
            " [[12 13 14 15]\n",
            "  [16 17 18 19]\n",
            "  [20 21 22 23]]]\n",
            "Shape of x_np: (2, 3, 4)\n",
            "\n",
            "NumPy array after .reshape(2, 12):\n",
            "[[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17 18 19 20 21 22 23]]\n",
            "Shape of x_np_reshape: (2, 12)\n",
            "\n",
            "NumPy array after .reshape(-1):\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n",
            "Shape of x_np_reshape_1d: (24,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "x_np = np.arange(24).reshape(2, 3, 4)\n",
        "print(\"Original NumPy array (x_np):\")\n",
        "print(x_np)\n",
        "print(\"Shape of x_np:\", x_np.shape)\n",
        "\n",
        "x_np_reshape = x_np.reshape(2, 12)\n",
        "print(\"\\nNumPy array after .reshape(2, 12):\")\n",
        "print(x_np_reshape)\n",
        "print(\"Shape of x_np_reshape:\", x_np_reshape.shape)\n",
        "\n",
        "x_np_reshape_1d = x_np.reshape(-1)\n",
        "print(\"\\nNumPy array after .reshape(-1):\")\n",
        "print(x_np_reshape_1d)\n",
        "print(\"Shape of x_np_reshape_1d:\", x_np_reshape_1d.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edcd0857"
      },
      "source": [
        "### Broadcasting in PyTorch\n",
        "\n",
        "Broadcasting is a mechanism in PyTorch (and NumPy) that allows element-wise operations on tensors of different shapes. It's a powerful feature that reduces the need for explicit loops or tile operations, making code more concise and often more efficient.\n",
        "\n",
        "**The Broadcasting Rules:**\n",
        "\n",
        "For two tensors to be broadcastable, the following rules must hold:\n",
        "\n",
        "1.  **Equal number of dimensions:** If the tensors do not have the same number of dimensions, the shape of the one with fewer dimensions is *padded* with ones on its leading (left) side.\n",
        "2.  **Compatibility of dimensions:** Starting from the trailing (rightmost) dimension, and moving left, the dimensions must either:\n",
        "    *   Be equal.\n",
        "    *   One of them is 1.\n",
        "\n",
        "If these rules are not met, the tensors are not broadcastable, and PyTorch will raise a `RuntimeError`.\n",
        "\n",
        "**Example Scenarios:**\n",
        "\n",
        "*   **Scalar and Tensor:** A scalar (0-D tensor) can be broadcast to any tensor shape.\n",
        "*   **1-D and N-D:** A 1-D tensor can be broadcast if its size matches one of the dimensions of the N-D tensor, often used for operations across rows or columns.\n",
        "*   **Different but compatible shapes:** Tensors with different numbers of dimensions or different dimension sizes (where one is 1) can be broadcast.\n",
        "\n",
        "Let's look at some examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3542c2a",
        "outputId": "be08b518-ad3a-44ed-8b1d-b8d458cb62f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a (scalar): tensor(5.) Shape: torch.Size([])\n",
            "b (3x3 tensor):\n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]]) Shape: torch.Size([3, 3])\n",
            "Result of a * b:\n",
            " tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.],\n",
            "        [5., 5., 5.]])\n",
            "c (1D tensor): tensor([1, 2, 3]) Shape: torch.Size([3])\n",
            "d (2D tensor):\n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]]) Shape: torch.Size([4, 3])\n",
            "Result of c + d:\n",
            " tensor([[2., 3., 4.],\n",
            "        [2., 3., 4.],\n",
            "        [2., 3., 4.],\n",
            "        [2., 3., 4.]])\n",
            "e (column vector):\n",
            " tensor([[10],\n",
            "        [20],\n",
            "        [30],\n",
            "        [40]]) Shape: torch.Size([4, 1])\n",
            "f (2D tensor):\n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]]) Shape: torch.Size([4, 3])\n",
            "Result of e * f:\n",
            " tensor([[10., 10., 10.],\n",
            "        [20., 20., 20.],\n",
            "        [30., 30., 30.],\n",
            "        [40., 40., 40.]])\n",
            "g (1x3 tensor):\n",
            " tensor([[1, 2, 3]]) Shape: torch.Size([1, 3])\n",
            "h (2x1 tensor):\n",
            " tensor([[10],\n",
            "        [20]]) Shape: torch.Size([2, 1])\n",
            "Result of g + h:\n",
            " tensor([[11, 12, 13],\n",
            "        [21, 22, 23]])\n",
            "i (1D tensor): tensor([1, 2]) Shape: torch.Size([2])\n",
            "j (3x3 tensor):\n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]]) Shape: torch.Size([3, 3])\n",
            "Caught expected error: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor(5.0)\n",
        "b = torch.ones(3, 3)\n",
        "\n",
        "\n",
        "print(\"a (scalar):\", a, \"Shape:\", a.shape)\n",
        "print(\"b (3x3 tensor):\\n\", b, \"Shape:\", b.shape)\n",
        "print(\"Result of a * b:\\n\", a * b)\n",
        "\n",
        "c = torch.tensor([1, 2, 3])\n",
        "d = torch.ones(4, 3)\n",
        "\n",
        "print(\"c (1D tensor):\", c, \"Shape:\", c.shape)\n",
        "print(\"d (2D tensor):\\n\", d, \"Shape:\", d.shape)\n",
        "print(\"Result of c + d:\\n\", c + d)\n",
        "\n",
        "e = torch.tensor([[10], [20], [30], [40]])\n",
        "f = torch.ones(4, 3)\n",
        "\n",
        "print(\"e (column vector):\\n\", e, \"Shape:\", e.shape)\n",
        "print(\"f (2D tensor):\\n\", f, \"Shape:\", f.shape)\n",
        "print(\"Result of e * f:\\n\", e * f)\n",
        "\n",
        "g = torch.tensor([[1, 2, 3]])\n",
        "h = torch.tensor([[10], [20]])\n",
        "\n",
        "print(\"g (1x3 tensor):\\n\", g, \"Shape:\", g.shape)\n",
        "print(\"h (2x1 tensor):\\n\", h, \"Shape:\", h.shape)\n",
        "print(\"Result of g + h:\\n\", g + h)\n",
        "\n",
        "\n",
        "i = torch.tensor([1, 2])\n",
        "j = torch.ones(3, 3)\n",
        "\n",
        "print(\"i (1D tensor):\", i, \"Shape:\", i.shape)\n",
        "print(\"j (3x3 tensor):\\n\", j, \"Shape:\", j.shape)\n",
        "try:\n",
        "    print(\"Result of i + j:\", i + j)\n",
        "except RuntimeError as err:\n",
        "    print(f\"Caught expected error: {err}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41c45e24"
      },
      "source": [
        "### In-place vs. Out-of-place Operations\n",
        "\n",
        "In programming, especially when dealing with data structures like tensors or arrays, operations can generally be categorized as either in-place or out-of-place.\n",
        "\n",
        "#### Out-of-place Operations (Default and generally safer)\n",
        "\n",
        "*   **Definition:** An out-of-place operation creates a *new* data structure (e.g., a new tensor or array) to store the result of the operation. The original data structure remains unchanged.\n",
        "*   **Characteristics:**\n",
        "    *   More memory-intensive, as a copy of the data is often created.\n",
        "    *   Safer, as the original data is preserved, preventing unintended side effects.\n",
        "    *   Easier for debugging, as you can inspect intermediate states.\n",
        "*   **Example:** `c = a + b` (where `a` and `b` are tensors/arrays). Here, `a` and `b` are not modified; a new tensor/array `c` is created with their sum.\n",
        "\n",
        "#### In-place Operations\n",
        "\n",
        "*   **Definition:** An in-place operation modifies the existing data structure directly, without creating a new one. The result overwrites the original content.\n",
        "*   **Characteristics:**\n",
        "    *   More memory-efficient, as no new memory is allocated for the result.\n",
        "    *   Can be faster in some cases due to reduced memory allocation/deallocation.\n",
        "    *   Less safe, as the original data is permanently altered, which can lead to unexpected behavior if not carefully managed.\n",
        "    *   Often denoted by a trailing underscore in PyTorch (e.g., `add_`, `mul_`) or assignment operators in Python/NumPy (e.g., `+=`).\n",
        "*   **Example:** `a.add_(b)` (in PyTorch) or `a += b` (in NumPy/PyTorch). Here, the tensor/array `a` is modified directly.\n",
        "\n",
        "**When to use which:**\n",
        "\n",
        "*   **Out-of-place:** Prefer out-of-place operations in most scenarios for clarity, safety, and ease of debugging. They are the default behavior for most arithmetic operations.\n",
        "*   **In-place:** Use in-place operations when memory efficiency is critical, or when you are certain that you no longer need the original value of the data structure and want to avoid unnecessary memory allocations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e7b1250",
        "outputId": "97e2f752-1b6e-4806-a292-b6abb1ef902b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor_a: tensor([1, 2, 3])\n",
            "Result of out-of-place addition (tensor_c): tensor([5, 7, 9])\n",
            "tensor_a after out-of-place operation: tensor([1, 2, 3])\n",
            "\n",
            "Original tensor_x: tensor([10, 20, 30])\n",
            "tensor_x after in-place add_ operation: tensor([11, 21, 31])\n",
            "Original tensor_z: tensor([100, 200, 300])\n",
            "tensor_z after in-place += operation: tensor([101, 201, 301])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tensor_a = torch.tensor([1, 2, 3])\n",
        "tensor_b = torch.tensor([4, 5, 6])\n",
        "print(f\"Original tensor_a: {tensor_a}\")\n",
        "\n",
        "tensor_c = tensor_a + tensor_b\n",
        "print(f\"Result of out-of-place addition (tensor_c): {tensor_c}\")\n",
        "print(f\"tensor_a after out-of-place operation: {tensor_a}\\n\")\n",
        "\n",
        "tensor_x = torch.tensor([10, 20, 30])\n",
        "tensor_y = torch.tensor([1, 1, 1])\n",
        "print(f\"Original tensor_x: {tensor_x}\")\n",
        "\n",
        "tensor_x.add_(tensor_y)\n",
        "print(f\"tensor_x after in-place add_ operation: {tensor_x}\")\n",
        "\n",
        "tensor_z = torch.tensor([100, 200, 300])\n",
        "print(f\"Original tensor_z: {tensor_z}\")\n",
        "\n",
        "tensor_z += tensor_y\n",
        "print(f\"tensor_z after in-place += operation: {tensor_z}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
