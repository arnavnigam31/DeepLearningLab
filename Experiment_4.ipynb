{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnavnigam31/DeepLearningLab/blob/main/Experiment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tBNUw9tyqRNY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "with open(\"poems-100.csv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read().lower()\n",
        "\n",
        "tokens = text.split()\n",
        "vocab = sorted(set(tokens))\n",
        "word2idx = {w:i for i,w in enumerate(vocab)}\n",
        "idx2word = {i:w for w,i in word2idx.items()}\n",
        "vocab_size = len(vocab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mEkSj_UqOOr"
      },
      "source": [
        "ONE-HOT ENCODING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "to0bct0LqVJk"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(idx, vocab_size):\n",
        "    vec = torch.zeros(vocab_size)\n",
        "    vec[idx] = 1.0\n",
        "    return vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjXPzxU2qdXq"
      },
      "source": [
        "Create Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lIrm6bc0qbET"
      },
      "outputs": [],
      "source": [
        "seq_length = 5\n",
        "X, y = [], []\n",
        "\n",
        "for i in range(len(tokens) - seq_length):\n",
        "    seq = tokens[i:i+seq_length]\n",
        "    target = tokens[i+seq_length]\n",
        "    X.append([word2idx[w] for w in seq])\n",
        "    y.append(word2idx[target])\n",
        "\n",
        "X = torch.tensor(X)\n",
        "y = torch.tensor(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lbXhDWKTvZfQ"
      },
      "outputs": [],
      "source": [
        "seq_length = X.shape[1]\n",
        "\n",
        "X_onehot = torch.zeros(X.shape[0], seq_length, vocab_size)\n",
        "\n",
        "for i in range(X.shape[0]):\n",
        "    for j in range(seq_length):\n",
        "        X_onehot[i, j, X[i, j]] = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oCQRINYFvcl7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "dataset = TensorDataset(X_onehot, y)\n",
        "loader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdy62Eg9qi16"
      },
      "source": [
        "RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9Rr5cLAYqf7I"
      },
      "outputs": [],
      "source": [
        "class RNN_OneHot(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(vocab_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-phqU43Yr3Z-",
        "outputId": "6cb3dc67-5ab8-4d0a-aeb4-f419052e2956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.4003\n",
            "Epoch 2, Loss: 6.7712\n",
            "Epoch 3, Loss: 6.4620\n",
            "Epoch 4, Loss: 6.1625\n",
            "Epoch 5, Loss: 5.9194\n",
            "Epoch 6, Loss: 5.6807\n",
            "Epoch 7, Loss: 5.4380\n",
            "Epoch 8, Loss: 5.1832\n",
            "Epoch 9, Loss: 4.9245\n",
            "Epoch 10, Loss: 4.6706\n",
            "Epoch 11, Loss: 4.4174\n",
            "Epoch 12, Loss: 4.1713\n",
            "Epoch 13, Loss: 3.9271\n",
            "Epoch 14, Loss: 3.6910\n",
            "Epoch 15, Loss: 3.4572\n",
            "Epoch 16, Loss: 3.2323\n",
            "Epoch 17, Loss: 3.0076\n",
            "Epoch 18, Loss: 2.7985\n",
            "Epoch 19, Loss: 2.5948\n",
            "Epoch 20, Loss: 2.4016\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 64\n",
        "model = RNN_OneHot(vocab_size, hidden_size)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        output = model(xb)\n",
        "        loss = criterion(output, yb)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CgP1mwXGo-7",
        "outputId": "cd124271-b95c-41c1-b6a8-574d37e057d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "the sun shines bright upon me for the large and work, is the welcome, of the day and eleves, i do at have forward! and the perpetual holding your oftener by the and is like the gamut, of these and are to the and fills the open and second of the lips, i am a\n"
          ]
        }
      ],
      "source": [
        "seed_words = [\"the\", \"sun\", \"shines\", \"bright\", \"upon\"]\n",
        "num_words_to_generate = 50\n",
        "def generate_text_for_onehot(model, seed_words, num_words_to_generate, word2idx, idx2word, vocab_size, seq_length):\n",
        "    model.eval()\n",
        "    generated_words = list(seed_words)\n",
        "\n",
        "    for _ in range(num_words_to_generate):\n",
        "        if len(generated_words) < seq_length:\n",
        "            current_sequence_words = generated_words\n",
        "        else:\n",
        "            current_sequence_words = generated_words[-seq_length:]\n",
        "\n",
        "        try:\n",
        "            input_indices = [word2idx[w] for w in current_sequence_words]\n",
        "        except KeyError as e:\n",
        "            print(f\"Warning: Word '{e.args[0]}' not in vocabulary. Skipping generation for this word.\")\n",
        "            break\n",
        "        input_tensor_indices = torch.tensor(input_indices).unsqueeze(0)\n",
        "        one_hot_input_tensor = torch.zeros(1, input_tensor_indices.shape[1], vocab_size, dtype=torch.float32)\n",
        "        for k, idx in enumerate(input_tensor_indices[0]):\n",
        "            one_hot_input_tensor[0, k, idx] = 1.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(one_hot_input_tensor)\n",
        "\n",
        "        predicted_idx = torch.argmax(output[:, -1, :]).item() if output.dim() == 3 else torch.argmax(output).item()\n",
        "        predicted_word = idx2word[predicted_idx]\n",
        "        generated_words.append(predicted_word)\n",
        "\n",
        "    return ' '.join(generated_words)\n",
        "generated_text = generate_text_for_onehot(model, seed_words, num_words_to_generate, word2idx, idx2word, vocab_size, seq_length)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoNz5AW9qmn3"
      },
      "source": [
        "LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-5l588lqqkg2"
      },
      "outputs": [],
      "source": [
        "class LSTM_OneHot(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(vocab_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOBHSotCqpc3",
        "outputId": "4e295aeb-c4e4-4172-aad9-ec1ef1b4ddcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.4664\n",
            "Epoch 2, Loss: 6.8816\n",
            "Epoch 3, Loss: 6.7495\n",
            "Epoch 4, Loss: 6.5955\n",
            "Epoch 5, Loss: 6.4220\n",
            "Epoch 6, Loss: 6.2191\n",
            "Epoch 7, Loss: 5.9873\n",
            "Epoch 8, Loss: 5.7252\n",
            "Epoch 9, Loss: 5.4407\n",
            "Epoch 10, Loss: 5.1328\n",
            "Epoch 11, Loss: 4.8236\n",
            "Epoch 12, Loss: 4.5168\n",
            "Epoch 13, Loss: 4.2187\n",
            "Epoch 14, Loss: 3.9311\n",
            "Epoch 15, Loss: 3.6549\n",
            "Epoch 16, Loss: 3.3869\n",
            "Epoch 17, Loss: 3.1273\n",
            "Epoch 18, Loss: 2.8777\n",
            "Epoch 19, Loss: 2.6353\n",
            "Epoch 20, Loss: 2.4036\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 64\n",
        "model = LSTM_OneHot(vocab_size, hidden_size)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        output = model(xb)\n",
        "        loss = criterion(output, yb)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "acBdyK3OGp7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc806721-ed79-4dbc-81b5-7b412a0c92e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "the sun shines bright upon no than and this take we to to me be a mind of still and all all and each they with the last of i love, the same of the first and of the men of all and and till then is my eyes a whatever of my things and\n"
          ]
        }
      ],
      "source": [
        "seed_words = [\"the\", \"sun\", \"shines\", \"bright\", \"upon\"]\n",
        "num_words_to_generate = 50\n",
        "\n",
        "generated_text = generate_text_for_onehot(model, seed_words, num_words_to_generate, word2idx, idx2word, vocab_size, seq_length)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zr2BLv9r7_f"
      },
      "source": [
        "Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "klPbwY3Mr-tk"
      },
      "outputs": [],
      "source": [
        "X = torch.tensor([[word2idx[w] for w in tokens[i:i+seq_length]]\n",
        "                  for i in range(len(tokens)-seq_length)])\n",
        "y = torch.tensor([word2idx[tokens[i+seq_length]]\n",
        "                  for i in range(len(tokens)-seq_length)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyxaiG50sBrW"
      },
      "source": [
        "RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1cxmG3DMr_D7"
      },
      "outputs": [],
      "source": [
        "class RNN_Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y3d04tAksE7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b50ccbc0-c65a-4aec-a403-b86a826a8ace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.4717\n",
            "Epoch 2, Loss: 6.6119\n",
            "Epoch 3, Loss: 6.2655\n",
            "Epoch 4, Loss: 5.9458\n",
            "Epoch 5, Loss: 5.6374\n",
            "Epoch 6, Loss: 5.3367\n",
            "Epoch 7, Loss: 5.0463\n",
            "Epoch 8, Loss: 4.7601\n",
            "Epoch 9, Loss: 4.4830\n",
            "Epoch 10, Loss: 4.2164\n",
            "Epoch 11, Loss: 3.9574\n",
            "Epoch 12, Loss: 3.7104\n",
            "Epoch 13, Loss: 3.4787\n",
            "Epoch 14, Loss: 3.2594\n",
            "Epoch 15, Loss: 3.0561\n",
            "Epoch 16, Loss: 2.8655\n",
            "Epoch 17, Loss: 2.6866\n",
            "Epoch 18, Loss: 2.5194\n",
            "Epoch 19, Loss: 2.3611\n",
            "Epoch 20, Loss: 2.2115\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "embedding_dataset = TensorDataset(X, y)\n",
        "embedding_loader = DataLoader(embedding_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "model = RNN_Embedding(vocab_size, embed_dim=100, hidden_size=64)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for xb, yb in embedding_loader:\n",
        "        output = model(xb)\n",
        "        loss = criterion(output, yb)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(embedding_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "d_Irj2izGrij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f83057-2e03-49aa-939e-3332326260bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "the sun shines bright upon their unsuccess. i see the people i have been stunn'd. stand back! is a man anyhow? what it shall be you! you sweaty brooks and dews it shall be you! you sweaty brooks and dews it shall be you! you sweaty brooks and dews it shall be you! you sweaty\n"
          ]
        }
      ],
      "source": [
        "seed_words = [\"the\", \"sun\", \"shines\", \"bright\", \"upon\"]\n",
        "num_words_to_generate = 50\n",
        "\n",
        "generated_text = generate_text(model, seed_words, num_words_to_generate, word2idx, idx2word, seq_length)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F70lqNFsL3F"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-JTKrsyXsM1p"
      },
      "outputs": [],
      "source": [
        "class LSTM_Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DCmi2WLrsPm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b244fed-6ca5-459f-d62a-75001eab8ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.4604\n",
            "Epoch 2, Loss: 6.6877\n",
            "Epoch 3, Loss: 6.4009\n",
            "Epoch 4, Loss: 6.1084\n",
            "Epoch 5, Loss: 5.8161\n",
            "Epoch 6, Loss: 5.5303\n",
            "Epoch 7, Loss: 5.2437\n",
            "Epoch 8, Loss: 4.9646\n",
            "Epoch 9, Loss: 4.6869\n",
            "Epoch 10, Loss: 4.4125\n",
            "Epoch 11, Loss: 4.1470\n",
            "Epoch 12, Loss: 3.8866\n",
            "Epoch 13, Loss: 3.6350\n",
            "Epoch 14, Loss: 3.3934\n",
            "Epoch 15, Loss: 3.1614\n",
            "Epoch 16, Loss: 2.9415\n",
            "Epoch 17, Loss: 2.7318\n",
            "Epoch 18, Loss: 2.5351\n",
            "Epoch 19, Loss: 2.3496\n",
            "Epoch 20, Loss: 2.1746\n"
          ]
        }
      ],
      "source": [
        "model = LSTM_Embedding(vocab_size, embed_dim=100, hidden_size=64)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for xb, yb in embedding_loader:\n",
        "        output = model(xb)\n",
        "        loss = criterion(output, yb)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(embedding_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "00c2889c"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, seed_words, num_words_to_generate, word2idx, idx2word, seq_length):\n",
        "    model.eval()\n",
        "    generated_words = list(seed_words)\n",
        "\n",
        "    for _ in range(num_words_to_generate):\n",
        "        if len(generated_words) < seq_length:\n",
        "            current_sequence_words = generated_words\n",
        "        else:\n",
        "            current_sequence_words = generated_words[-seq_length:]\n",
        "        try:\n",
        "            input_indices = [word2idx[w] for w in current_sequence_words]\n",
        "        except KeyError as e:\n",
        "            print(f\"Warning: Word '{e.args[0]}' not in vocabulary. Skipping generation for this word.\")\n",
        "            break\n",
        "\n",
        "\n",
        "        input_tensor = torch.tensor(input_indices).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "\n",
        "\n",
        "        predicted_idx = torch.argmax(output[:, -1, :]).item() if output.dim() == 3 else torch.argmax(output).item()\n",
        "        predicted_word = idx2word[predicted_idx]\n",
        "        generated_words.append(predicted_word)\n",
        "\n",
        "    return ' '.join(generated_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "b8dbd9f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efcdd69c-089c-446d-90e9-50ded3ab7244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "the sun shines bright upon the shore, death-messages on a veil the wheel'd universe, or the mate of his own sorrows) he on the old and liquid and height my heart and i. iii. we had burst at my heart and i! we have no chair, no little no philosophy, i heard that i will\n"
          ]
        }
      ],
      "source": [
        "seed_words = [\"the\", \"sun\", \"shines\", \"bright\", \"upon\"]\n",
        "num_words_to_generate = 50\n",
        "\n",
        "generated_text = generate_text(model, seed_words, num_words_to_generate, word2idx, idx2word, seq_length)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kJRWycIIlp4K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMMz22hLYH3tJr7GTdv/Vef",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}